{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8509faeb",
   "metadata": {},
   "source": [
    "# EXPLORATION 4: Lyricist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0bfb70",
   "metadata": {},
   "source": [
    "EX4 정도 진행을 하니 코딩의 기록을 어떻게 해야 하는지 좀 틀이 잡히는 듯 하다.  \n",
    "프로그래밍을 진행하면서 나의 의식의 흐름으로 작성을 하는것이 가장 좋을 듯 하다.  \n",
    "내가 무엇을 몰랐고, 무엇을 알려고 노력했으며, 어떤식으로 이해하고 넘어 갔는지를 기록해 볼 수 있다.  \n",
    "목표한 결과가 나오기까지의 전체적인 과정들을 알 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1472a",
   "metadata": {},
   "source": [
    "## (1) 패키지들을 import 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af81ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언제나 그렇듯 사용할 패키지 a.k.a 모듈 또는 라이브러리들을 불러 온다.\n",
    "\n",
    "import glob    # txt파일 읽어오는 glob이라는 패키지를 사용해 보도록 하자.\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ac371",
   "metadata": {},
   "source": [
    "## (2) 데이터 파일을 읽어들이자 (오늘의 데이터는 텍스트 데이터)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5b941",
   "metadata": {},
   "source": [
    "변하지 진리, 좋은 데이터는 좋은 모델을 만든다.  \n",
    "데이터를 잘 이해하고 다듬는 것은 좋은 모델을 만드는 첫 걸음이다.\n",
    "자 그러면 일단 데이터 파일부터 읽어들이자, 시작이 반이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ceaf0d",
   "metadata": {},
   "source": [
    "- ### 폴더에 존재하는 모든 텍스트 파일의 리스트를 읽어들이자.\n",
    "\n",
    "(0) 한동안 이미지 파일만 가지고 프로젝트를 했더니 일반 파읽 읽기가 좀 생소하게 느껴진다.  \n",
    "(1) corpus는 '말뭉치'라는 표현이다. raw_corpus에 방금불러들인 날것의 '말뭉치'들(텍스트 데이터)들을 저장하겠다는 의미  \n",
    "(2) 폴어 안에 있는 모든 파일의 경로를 glob을 사용하여 읽어들인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8804181",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/workplace/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = [] # 모든 것은 이 변수로 부터 시작된다. 집중하자, 여기에 raw data를 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21687809",
   "metadata": {},
   "source": [
    "- ### 일단 다음으로 넘어가기 전에 슬쩍 뭐가 들어있는지, 데이터 타입은 무엇인지 확인부터 하고 넘어가자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffadc745",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/aiffel/aiffel/workplace/lyricist/data/lyrics/leonard-cohen.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/lil-wayne.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/blink-182.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/bruce-springsteen.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/notorious_big.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/beatles.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/bob-dylan.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/alicia-keys.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/dr-seuss.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/radiohead.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/nickelback.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/prince.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/joni-mitchell.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/missy-elliott.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/dj-khaled.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/michael-jackson.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/janisjoplin.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/al-green.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/bruno-mars.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/cake.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/patti-smith.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/ludacris.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/lorde.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/kanye.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/Lil_Wayne.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/bjork.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/lin-manuel-miranda.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/dolly-parton.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/notorious-big.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/dickinson.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/Kanye_West.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/nirvana.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/britney-spears.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/disney.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/bieber.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/kanye-west.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/nicki-minaj.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/nursery_rhymes.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/paul-simon.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/bob-marley.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/amy-winehouse.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/rihanna.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/jimi-hendrix.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/lady-gaga.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/r-kelly.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/adele.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/johnny-cash.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/drake.txt',\n",
       " '/aiffel/aiffel/workplace/lyricist/data/lyrics/eminem.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test list 한번 확인해 보고 넘어가자 무엇이 들었을까?\n",
    "\n",
    "print(type(txt_list)) # 데이터 타입도 확인해주고, 예상한대로 리스트형\n",
    "txt_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea8415",
   "metadata": {},
   "source": [
    "- ### 텍스트 파일들의 내용을 모두 읽어 raw_corpus에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea5e92",
   "metadata": {},
   "source": [
    "LMS에서 실습한 것은 텍스트 파일 하나에 담긴 내용이라 읽은 후 바로 raw_corpus에 담을 수 있었지만  \n",
    "지금은 여러개의 파일에서 내용을 모두 읽어들여야 한다.  \n",
    "그래서 일단 데이터 폴더에 존재하는 텍스트 파일들의 제목을 모두 읽어서 그것을 리스트 (txt_list)로 저장하고  \n",
    "그 리스트를 기반으로 각 파일의 내용을 읽어서 raw_corpus에 저장 하도록 코딩하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eaaa31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러개의 txt 파일의 내용을 모두 읽어서 raw_corpus에 담는다.\n",
    "# close를 빼먹는 것을 방지하기 위해서 항상 파일을 오픈 할때는 with open() as X 라는 표현을 쓰는 것을 습관하 하도록 하자.\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:  # f 로 부르자\n",
    "        raw = f.read().splitlines() # 한줄씩 읽어들이자 --> splitline()\n",
    "        raw_corpus.extend(raw)      # 결국 txt_list에 있는 모든 파일들을 다 읽어 들여서 raw_corpus에 iterable 자료형인 리스트 형으로 저장하라는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310c543",
   "metadata": {},
   "source": [
    "- ### 로딩된 텍스트 데이터 내용 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d5bac2",
   "metadata": {},
   "source": [
    "(1) 잘 불러들여 졌는지 확인하기 위해서 앞에 몇개만 출력해 본다.  \n",
    "(2) 잘 불러 들여진듯 하니.... 일단 다음으로 패스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d0d0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "322f62f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_corpus # 전체 데이터도 한번 확인해 보고, Hallelujah는 제거해야 하는게 아닐까 하는 생각이든다. 데이터가 너무 길게 출력되어 실행결과는 생략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a66363c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth', 'The minor fall, the major lift', 'The baffled king composing Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah']\n"
     ]
    }
   ],
   "source": [
    "print(raw_corpus[:9]) # 대신 테스트로 앞에서 9개만 출력해본다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb46ecb",
   "metadata": {},
   "source": [
    "## (3) 텍스트 데이터 필터링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa53936",
   "metadata": {},
   "source": [
    "### 문장 정제 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f46de",
   "metadata": {},
   "source": [
    "LMS 샘플과는 상황이 다르다\n",
    "데이터를 확인해보면 기존의 규칙에서 추가 되어야 하는 부분이 있다는 것을 확인할 수 있다.  \n",
    "예를들면 I've don't won't 와 같은 것은 한 단어로 만들어 줘야 한다.  \n",
    "하지만 LMS에서 사용한 기존의 규칙을 그대로 복사해서 사용하면 '도 특수문자로 취급하기 때문에  \n",
    "양쪽에 공백을 만들고 I ve don t won t 이런식으로 단어를 분리해서 이상한 단어를 만들어 버린다.  \n",
    "해당 단어가 손상되지 않고 정확하게 남도록 코드를 수정해야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b426f",
   "metadata": {},
   "source": [
    "- 원하는 데이터를 얻기 위해서 어떤 형태로 데이터를 가공 또는 필터링 해야 하는지 확인해야 한다. 필터링 해야 하는 문장이나 단어들의 특징을 찾아 낸다.  \n",
    "\n",
    "- enumerate() 리스트나 문자열의 index값을 출력하고 싶을경우에 사용한다.  \n",
    "- for문의 경우에는 이 인덱스 값을 사용해야 하므로 enumerate를 사용한다.  \n",
    "- 우리는 문자열을 사용하고 있기 때문에 enumerate를 사용하여 반복문을 돌리면 된다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9bcbda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?'.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다. 여기에서 ' 기호를 추가 하였다.\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?'.!,¿]+\", \" \", sentence) # 4 '를 추가하여 위에서 언급한 단어들을 보존하였다. \n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8afd99",
   "metadata": {},
   "source": [
    "### 정제된 문장을 모으자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c528d77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<start> now i've heard there was a secret chord <end>\",\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " \"<start> but you don't really care for music , do you ? <end>\",\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    #if len(sentence) == 0: continue\n",
    "    #if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942731e9",
   "metadata": {},
   "source": [
    "#### 정규표현식을 사용해서 이상한 단어를 찾아서 바꾸기 작업을 하려고 하는데 일단 코드가 어려워서 나중에 작업예정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680eae0",
   "metadata": {},
   "source": [
    "전체 데이터 확인중에 You?d 과 같은 문장이 보인다. You'd 같은데 뭔가 잘못된 것이 있는듯 하다.  \n",
    "이런 것들이 몇개 보인다. 해당 형식을 가지고 있는 단어를 찾아서 정상적으로 바꾸는 코드를 만들려고 하는데 아직 구현을 못했다.  \n",
    "일단은 그대로 사용하여 학습 시킬예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "77dc2f3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find_word = 'You?d'\n",
    "\n",
    "#result = [i for i in range(len(raw_corpus)) if find_word in raw_corpus[i]]\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "58d53076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = re.compile('[a-zA-Z]+[?][a-zA-Z]+')\n",
    "#m = p.match(raw_corpus[202])\n",
    "\n",
    "\n",
    "#result = [i for i in range(len(raw_corpus)) if find_word in raw_corpus[i]]\n",
    "#result = [i for i in range(len(raw_corpus)) if p in raw_corpus[i]]\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e97dfa",
   "metadata": {},
   "source": [
    "### Tokenizer 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250e0a3",
   "metadata": {},
   "source": [
    "maxlen을 설정하지 않고 가장 긴문장으로 맞추는 디폴트 옵션으로 설정하면 1 epoch당 47분의 학습시간이 걸린다.  \n",
    "총 10 epoch를 돌리게 되면 470분의 약 8시간이 걸린다.  \n",
    "이는 상황상 무리가 있기 때문에 maxlen을 15로 설정하고 프로젝트를 진행하였다.  \n",
    "\n",
    "전체단어의 수를 12,000개로 변경하였고 maxlen은 15로 설정하였다,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0afe70bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   46  149 ...    0    0    0]\n",
      " [   2   15 2768 ...    0    0    0]\n",
      " [   2   32    7 ...    3    0    0]\n",
      " ...\n",
      " [   2  129   20 ...   10 1040    3]\n",
      " [8545    6   35 ... 1312  656    3]\n",
      " [   2    7   30 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fcedaf3f5e0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,   # 전체 단어의 개수 \n",
    "        filters=' ',       # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "13b72db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   46  149  302   86   53    9 1006 6212    3    0    0    0    0]\n",
      "[  46  149  302   86   53    9 1006 6212    3    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d79fd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b1f3185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (149670, 14)\n",
      "Target Train: (149670, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                    tgt_input, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=7)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e0849fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512 # 워드 벡터의 차원수, 단어가 추상적으로 표현되는 크기 \n",
    "hidden_size = 1024   # 얼마나 많은 일꾼을 둘 것인가?\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fee9e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the save point\n",
    "checkpoint_dir = os.getenv('HOME')+'/aiffel/workplace/lyricist/models/lyricist'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 mode='auto',\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45435eba",
   "metadata": {},
   "source": [
    "### 훈련결과 loss = 1.9534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d597053d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "730/730 [==============================] - 141s 188ms/step - loss: 3.4005\n",
      "Epoch 2/10\n",
      "730/730 [==============================] - 140s 191ms/step - loss: 2.9567\n",
      "Epoch 3/10\n",
      "730/730 [==============================] - 141s 192ms/step - loss: 2.7652\n",
      "Epoch 4/10\n",
      "730/730 [==============================] - 140s 191ms/step - loss: 2.6166\n",
      "Epoch 5/10\n",
      "730/730 [==============================] - 140s 192ms/step - loss: 2.4861\n",
      "Epoch 6/10\n",
      "730/730 [==============================] - 141s 192ms/step - loss: 2.3669\n",
      "Epoch 7/10\n",
      "730/730 [==============================] - 140s 192ms/step - loss: 2.2558\n",
      "Epoch 8/10\n",
      "730/730 [==============================] - 140s 192ms/step - loss: 2.1503\n",
      "Epoch 9/10\n",
      "730/730 [==============================] - 141s 192ms/step - loss: 2.0499\n",
      "Epoch 10/10\n",
      "730/730 [==============================] - 141s 192ms/step - loss: 1.9534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcf846c7820>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "92ace9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "        tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다.\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c22ca",
   "metadata": {},
   "source": [
    "### good을 넣고 돌려본 결과는 다음과 같이 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7bc693",
   "metadata": {},
   "source": [
    "### good morning , give me a million reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "43d707ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> good morning , give me a million reasons <end> '"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> good\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f046f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a23f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6eafda7",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566371d4",
   "metadata": {},
   "source": [
    "이번 EX04는 지금까지 진행한 프로젝트중에서 제일 까다로웠다.  \n",
    "\n",
    "1. 데이터의 특성도 최대한 자세하게 고려하기 위해서 까다롭게 살펴 보는 과정에 공을 많이 들였다.  \n",
    "이때 I've don't와 같은 특성을 발견해서 손상되지 않도록 필터링을 하였다. \n",
    "또한 문자들중 You?d 와 같이 표현된 다수의 문자들을 발견해서 정규표현식을 사용하여 You'd와 같이 수정하려고 했지만 해당 부분은 조금 더 공부를 해야 할 것 같다는 판단에 일단 중지하고 프로젝트를 계속 진행하였다. 정규표현식을 사용해서 특정한 조건의 문자를 반복적으로 교체하는 부분에 대한 이해가 필요하다.\n",
    "\n",
    "\n",
    "2. 사실은 모든 데이터를 사용하여 모델을 만들기 위해서. Tokenizer() 함수에서 max_len을 디폴트 값으로 설정하고 훈련을 했으나 시간이 너무 오래 걸리는 상황이 되었다. 다른 변수들을 수정하여도 해당 시간은 크게 달라지지 않았다. 하여 max_len을 15로 변경한 상태에서 훈련을 진행하였다. 추후에 이부분에 대해서는 다시한번 확인하고 점검하는 것이 필요하다고 생각된다. \n",
    "\n",
    "\n",
    "3. 해당 모델에 대한 이론이 부족한 상태에서 프로젝트를 진행하니 역부족이라는 생각이 들었다. 일단은 큰 그림을 먼저 본다고 생각하고 무리 없이 작동하는 것에 촛점을 맞추어 진행하였다. \n",
    "\n",
    "\n",
    "4. 데이터 전처리에 대한 기능적인 부분 방법론 적인 부분에 대한 심도있는 공부가 추가적으로 필요할 듯 싶다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28264a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d8577f7",
   "metadata": {},
   "source": [
    "## Study for EX4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76600e54",
   "metadata": {},
   "source": [
    "이곳은 EX4를 진행하면서 필요한 공부를 진행한 흔적을 남기는 공간입니다.  \n",
    "추후 내가 무엇을 몰랐고 무엇을 헛갈려 했는지 확인할 수 있습니다.\n",
    "기록은 실력입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906e267",
   "metadata": {},
   "source": [
    "### splitlines() 함수 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f4c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "traveler = \"\"\"강나루 건너서\\n밀밭 길을\\n\\n구름에 달 가듯이\\n가는 나그네\\n\n",
    "\n",
    "길은 외줄기\\n남도 삼백리\\n\\n술 익는 마을마다\\n타는 저녁놀\\n\n",
    "\n",
    "구름에 달 가듯이\\n가는 나그네\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeda8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "poet = traveler.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a85bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['강나루 건너서', '밀밭 길을', '', '구름에 달 가듯이', '가는 나그네', '', '', '길은 외줄기', '남도 삼백리', '', '술 익는 마을마다', '타는 저녁놀', '', '', '구름에 달 가듯이', '가는 나그네']\n"
     ]
    }
   ],
   "source": [
    "print(poet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c07dd274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강나루 건너서\n",
      "밀밭 길을\n",
      "\n",
      "구름에 달 가듯이\n",
      "가는 나그네\n",
      "\n",
      "\n",
      "길은 외줄기\n",
      "남도 삼백리\n",
      "\n",
      "술 익는 마을마다\n",
      "타는 저녁놀\n",
      "\n",
      "\n",
      "구름에 달 가듯이\n",
      "가는 나그네\n"
     ]
    }
   ],
   "source": [
    "for line in poet:\n",
    "\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e5c61",
   "metadata": {},
   "source": [
    "### 정제 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5d1ed861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e23924f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45265942 0.46149126 0.66491722 0.57913661]\n",
      " [0.84503418 0.61600919 0.09576326 0.88066692]\n",
      " [0.18162063 0.35083383 0.64304336 0.99273951]\n",
      " [0.63371814 0.8565983  0.20265233 0.49946737]\n",
      " [0.09757024 0.0536169  0.82579491 0.16102758]\n",
      " [0.48184313 0.40398499 0.6522637  0.59592203]\n",
      " [0.11996699 0.43222936 0.27585619 0.89409328]\n",
      " [0.98788334 0.83711884 0.16414894 0.61062295]\n",
      " [0.66037826 0.26647478 0.05190215 0.56254569]\n",
      " [0.34664122 0.55441537 0.86177822 0.95355645]]\n",
      "tf.Tensor([0.45265942 0.46149126 0.66491722 0.57913661], shape=(4,), dtype=float64)\n",
      "tf.Tensor([0.84503418 0.61600919 0.09576326 0.88066692], shape=(4,), dtype=float64)\n",
      "tf.Tensor([0.18162063 0.35083383 0.64304336 0.99273951], shape=(4,), dtype=float64)\n",
      "tf.Tensor([0.63371814 0.8565983  0.20265233 0.49946737], shape=(4,), dtype=float64)\n",
      "tf.Tensor([0.09757024 0.0536169  0.82579491 0.16102758], shape=(4,), dtype=float64)\n",
      "tf.Tensor([0.48184313 0.40398499 0.6522637  0.59592203], shape=(4,), dtype=float64)\n",
      "tf.Tensor([0.11996699 0.43222936 0.27585619 0.89409328], shape=(4,), dtype=float64)\n",
      "tf.Tensor([0.98788334 0.83711884 0.16414894 0.61062295], shape=(4,), dtype=float64)\n",
      "tf.Tensor([0.66037826 0.26647478 0.05190215 0.56254569], shape=(4,), dtype=float64)\n",
      "tf.Tensor([0.34664122 0.55441537 0.86177822 0.95355645], shape=(4,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x = np.random.sample((10, 4))\n",
    "print(x)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "#type(dataset)\n",
    "\n",
    "for i in dataset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8472280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
    "\n",
    "for i in dataset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40bbfa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.05728447 0.7002839  0.3504312  0.16974461 0.08753824 0.49075675\n",
      " 0.034549   0.68586886 0.6436845  0.61786485], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0.7744615  0.7309557  0.34500647 0.00230932 0.9476961  0.3632636\n",
      " 0.6413373  0.739557   0.21821117 0.4731332 ], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0.5883421  0.62269795 0.7505076  0.7154732  0.77434456 0.08255303\n",
      " 0.27171493 0.07187235 0.24762654 0.12759483], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0.37824535 0.85496557 0.37605762 0.43130946 0.9210603  0.2741623\n",
      " 0.0866679  0.96838987 0.29742897 0.89832604], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
    "\n",
    "for i in dataset1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad6c9e",
   "metadata": {},
   "source": [
    "### strip() 함수에 대한 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a63da41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_str = \"                   hello         \"\n",
    "ex_str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e592bd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now I heard there was a secret chord'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Now I heard there was a secret chord '.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0daefe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But you don't really care for music ,  do you ? \n"
     ]
    }
   ],
   "source": [
    "result = re.sub(r\"([?.!,¿])\", r\" \\1 \", \"But you don't really care for music, do you?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176f33b",
   "metadata": {},
   "source": [
    "### batch size가 대체 무엇인지몰라서 찾아본 자료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eecdf3",
   "metadata": {},
   "source": [
    "https://m.blog.naver.com/qbxlvnf11/221449297033"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
